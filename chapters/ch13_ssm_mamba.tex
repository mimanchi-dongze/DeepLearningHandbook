\chapter{状态空间模型 (SSM \& Mamba)}
\label{ch:ssm_mamba}

状态空间模型（State Space Models，如 S4、Mamba）是试图替代 Transformer 作为序列建模基石的架构，它结合了 RNN 的推理效率和 CNN 的训练并行性。

\section{连续系统数学模型 (ODE)}

\begin{variable}
\begin{itemize}
    \item $h(t) \in \mathbb{R}^N$: 隐藏状态，代表系统的记忆，$N$ 为状态维度。
    \item $\mathbf{A} \in \mathbb{R}^{N \times N}$: 演化矩阵，控制记忆的衰减与转换。
    \item $\mathbf{B} \in \mathbb{R}^{N \times 1}$: 输入矩阵，将一维输入映射到 $N$ 维状态空间。
    \item $\mathbf{C} \in \mathbb{R}^{1 \times N}$: 输出矩阵，将状态空间重新映射回输出信号。
    \item $\Delta$: 离散化步长，控制采样的时间密度。
\end{itemize}
\end{variable}

状态空间模型本质上是将输入信号 $x(t) \in \mathbb{R}$ 映射到隐藏状态 $h(t) \in \mathbb{R}^N$，再投影到输出 $y(t) \in \mathbb{R}$ 的常微分方程。

\begin{rosetta}{连续域常微分方程}
    \textbf{状态方程}:
    \begin{equation}
        h'(t) = \mathbf{A}h(t) + \mathbf{B}x(t)
    \end{equation}
    \textbf{输出方程}:
    \begin{equation}
        y(t) = \mathbf{C}h(t) + \mathbf{D}x(t)
    \end{equation}
    其中 $\mathbf{A} \in \mathbb{R}^{N \times N}$ 是演化矩阵，通常使用 HiPPO（High-order Polynomial Projection Operators）初始化，以确保系统能够“记住”历史。
\end{rosetta}

\section{离散化 (Discretization: ZOH)}

在实际的深度学习处理中，信号是离散的文本 Token（时间间隔 $\Delta$）。我们需要使用零阶保持（Zero-Order Hold, ZOH）将其离散化为类似 RNN 的差分方程形式。

\begin{rosetta}{ZOH 离散变换}
    给定步长 $\Delta$，离散系统参数 $\bar{\mathbf{A}}$ 和 $\bar{\mathbf{B}}$ 的严格数学推导为：
    \begin{align}
        \bar{\mathbf{A}} &= \exp(\Delta \mathbf{A}) \\
        \bar{\mathbf{B}} &= (\Delta \mathbf{A})^{-1} (\exp(\Delta \mathbf{A}) - I) \cdot \Delta \mathbf{B}
    \end{align}
    \textbf{离散递推公式}:
    \begin{align}
        h_k &= \bar{\mathbf{A}}h_{k-1} + \bar{\mathbf{B}}x_k \\
        y_k &= \mathbf{C}h_k
    \end{align}
    *(省略 D 项，通常在网络中视为残差连接。)*
\end{rosetta}

\section{选择性扫描 (Mamba: Selective Scan)}

在传统的 S4 中，$\bar{\mathbf{A}}, \bar{\mathbf{B}}$ 对于所有时间步是**时不变的（LTI）**，因此可以通过 FFT（快速傅里叶变换）进行卷积加速。
而 Mamba 的核心突破在于**数据相关性（Data-Dependent）**：让 $\Delta, \mathbf{B}, \mathbf{C}$ 成为输入 $x_t$ 的函数，从而能够选择性地过滤或记住信息（类似于 LSTM 的遗忘门）。

\begin{center}
\begin{tikzpicture}[node distance=1.5cm, font=\sffamily\small]
    % 输入
    \node[data_node] (x) {$x_t$};
    
    % 参数生成
    \node[block, right=1.5cm of x] (linear_delta) {$\text{Linear}_{\Delta}$};
    \node[block, above=0.5cm of linear_delta] (linear_b) {$\text{Linear}_{B}$};
    \node[block, below=0.5cm of linear_delta] (linear_c) {$\text{Linear}_{C}$};
    
    \draw[conn] (x) -- (linear_delta);
    \draw[conn] (x) |- (linear_b);
    \draw[conn] (x) |- (linear_c);
    
    % 生成的参数
    \node[active_neuron, right=1cm of linear_delta] (delta) {$\Delta_t$};
    \node[active_neuron, right=1cm of linear_b] (bt) {$B_t$};
    \node[active_neuron, right=1cm of linear_c] (ct) {$C_t$};
    
    \draw[data_flow] (linear_delta) -- (delta);
    \draw[data_flow] (linear_b) -- (bt);
    \draw[data_flow] (linear_c) -- (ct);
    
    % SSM 核心
    \node[block, right=1.5cm of delta, fill=brandblue!20] (ssm) {Selective SSM \\ $h_t = \bar{A}h_{t-1} + \bar{B}x_t$};
    \draw[conn] (delta) -- (ssm);
    \draw[conn] (bt) -| (ssm);
    \draw[conn] (ct) -| (ssm);
    \draw[conn] (x) -- ++(0,-2) -| (ssm);

    % 背景容器
    \begin{scope}[on background layer]
        \node[container, fit=(linear_delta) (linear_b) (linear_c) (delta) (bt) (ct)] (selection_logic) {};
        \node[anchor=south west, brandorange!50] at (selection_logic.north west) {Selection Mechanism};
    \end{scope}
\end{tikzpicture}
\end{center}

\begin{rosetta}{Mamba 选择性机制}
    由于时变特性，FFT 不再适用。Mamba 使用高效的前缀和扫描算法（Hardware-aware Scan）：
    \begin{equation}
        \mathbf{B}_t = \text{Linear}_{B}(x_t), \quad \mathbf{C}_t = \text{Linear}_{C}(x_t)
    \end{equation}
    \begin{equation}
        \Delta_t = \text{Softplus}(\text{Linear}_{\Delta}(x_t))
    \end{equation}
    此时 $h_t$ 的递推变为时变的：
    \begin{equation}
        h_t = \bar{\mathbf{A}}_t h_{t-1} + \bar{\mathbf{B}}_t x_t
    \end{equation}
\end{rosetta}
